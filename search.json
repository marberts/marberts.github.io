[
  {
    "objectID": "blog/posts/2025/multilaterals/index.html",
    "href": "blog/posts/2025/multilaterals/index.html",
    "title": "Making big multilateral indexes fast",
    "section": "",
    "text": "Multilateral price indexes are often used to measure the evolution of prices over time when there are large volumes of transaction data, such as retail scanner data or housing data. The main challenge with computing multilateral indexes with large amounts of data is that these indexes often depend on a matrix where the dimensions are at least as large as the number of products. Lots of data means lots of different products, which in turns means making quite large matrices. Indeed, assuming these matrices are stored as 64-bit floats (more on that below), at least \\(8k^2\\) bytes of memory are needed just to store one of these matrices with \\(k\\) products‚Äîwith, say, 100,000 products, that‚Äôs at least 80 GB of memory before doing any computations. Effectively computing these indexes requires a way to reduce the dimensions of these matrices."
  },
  {
    "objectID": "blog/posts/2025/multilaterals/index.html#weighted-time-product-dummy",
    "href": "blog/posts/2025/multilaterals/index.html#weighted-time-product-dummy",
    "title": "Making big multilateral indexes fast",
    "section": "Weighted time product dummy",
    "text": "Weighted time product dummy\nLet‚Äôs start with the WTPD index. This index is recovered from the coefficient on a set of time-dummy variables in a weighted log-linear regression model with a dummy variable for each product (e.g., Diewert and Fox 2022, eq. 9). Fitting this model is impractical with many products‚Äîwith \\(n\\) observations for \\(k\\) products over \\(t\\) time periods, the design matrix is a \\(n \\times (k + t - 1)\\) matrix and will consume at least \\(8(k + t - 1)^2\\) bytes of memory. Fortunately, only the coefficients on the time-dummy variables are required to make the index and so the product-dummy variables can be removed by treating them as fixed effects and demeaning the regression equation (Wooldridge 2002, sec. 10.5). Now the design matrix has only \\(t - 1\\) columns and, as \\(t\\) is usually much smaller than \\(k\\), will consume far less memory.\nLet‚Äôs consider a example. We‚Äôll start with a function to make (unbalanced) transaction data for a collection of products over time.\n\nset.seed(1552443)\n\nmake_prices &lt;- function(products, periods, frac = 0.1) {\n  res &lt;- data.frame(\n    product = as.factor(seq_len(products)),\n    time = as.factor(rep(seq_len(periods), each = products)),\n    price = rlnorm(products * periods),\n    quantity = runif(products * periods)\n  )\n  res[-sample(nrow(res), frac * nrow(res)), ]\n}\n\nhead(make_prices(10, 13))\n\n  product time     price  quantity\n2       2    1 1.4944618 0.2978414\n3       3    1 0.6778284 0.9263743\n5       5    1 1.2611687 0.3008513\n7       7    1 0.7996127 0.9532272\n8       8    1 0.2138277 0.4409094\n9       9    1 1.9830544 0.9422246\n\n\nWe‚Äôll also make a function to take these data and construct the WTPD index from a fixed-effects model with the {fixest} package.\n\nwtpd_index &lt;- function(prices) {\n  prices &lt;- prices |&gt;\n    dplyr::group_by(time) |&gt;\n    dplyr::mutate(w = gpindex::scale_weights(price * quantity))\n\n  mdl &lt;- fixest::feols(\n    log(price) ~ time | product,\n    data = prices,\n    weights = ~w\n  )\n\n  exp(coef(mdl))\n}\n\nNow let‚Äôs build a WTPD index with data for 100,000 products over 25 time periods. Doing this with a regular linear model, where each product has its own dummy variable, would require 1.8 TB of memory to just store the design matrix, making it impractical to calculate. Using a fixed-effects model to recover the WTPD index can be done without issue on any decent laptop.\n\nprice_data &lt;- make_prices(1e5, 25)\n\nbench::mark(fixest = wtpd_index(price_data))\n\n# A tibble: 1 √ó 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fixest        4.17s    4.17s     0.240    2.77GB    0.960\n\n\nNote that this is generally much faster than the routine for making the WTPD index in the excellent {IndexNumR} package.\n\nwtpd_index2 &lt;- function(prices, window) {\n  # IndexNumR expects these to be integers.\n  prices[c(\"time\", \"product\")] &lt;- lapply(\n    prices[c(\"time\", \"product\")],\n    as.integer\n  )\n  res &lt;- IndexNumR::WTPDIndex(\n    prices,\n    \"price\",\n    \"quantity\",\n    \"time\",\n    \"product\",\n    window = window\n  )\n  as.numeric(res[-1])\n}\n\nprice_data &lt;- make_prices(50, 25)\n\nbench::mark(\n  fixest = unname(wtpd_index(price_data)),\n  IndexNumR = wtpd_index2(price_data, 25)\n)\n\n# A tibble: 2 √ó 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fixest       8.77ms   9.56ms   103.       1.51MB     3.98\n2 IndexNumR     3.59s    3.59s     0.278  102.15MB     8.07"
  },
  {
    "objectID": "blog/posts/2025/multilaterals/index.html#geary-khamis-index",
    "href": "blog/posts/2025/multilaterals/index.html#geary-khamis-index",
    "title": "Making big multilateral indexes fast",
    "section": "Geary-Khamis index",
    "text": "Geary-Khamis index\nThe GK index is another multilateral index that‚Äôs often written as a function of a large matrix. Following Diewert and Fox (2022), the GK index can be written as a function of two \\(k \\times k\\) matrices. One of these matrices is diagonal, so it can be handled efficiently, but the other is usually dense (unless product imbalanced is really bad) and is impractical to store in memory. Using the same data as with the WTPD index, it would require 80 GB of memory to just store this matrix.\nAn alternative approach (Balk 2008, 245‚Äì46) expresses the GK index as a function of three \\(t \\times t\\) matrices. With the same data as before, this now requires only 15 KB of memory. Here is relatively fast function to implement this approach.\n\n#' Intertemporal Geary-Khamis index\n#'\n#' Make a multilateral Geary-Khamis price index.\n#'\n#' @param p A numeric vector of prices.\n#' @param q A numeric vector of quantities.\n#' @param period A factor, or something that can be coerced into one, that\n#'   gives the corresponding time period for each element in `p` and\n#'   `q`. The ordering of time periods follows the levels of `period`\n#'   to agree with [`cut()`][cut.Date].\n#' @param product A factor, or something that can be coerced into one, that\n#'   gives the corresponding product identifier for each element in `p` and\n#'   `q`.\n#' @param na.rm Should missing values be removed? By default they are not.\n#'\n#' @returns\n#' A numeric vector of period-over-period price indexes.\n\nfast_gk &lt;- function(p, q, period, product, na.rm = FALSE) {\n  period &lt;- as.factor(period)\n  product &lt;- as.factor(product)\n\n  qs &lt;- unsplit(lapply(split(q, product), gpindex::scale_weights), product)\n  attributes(product) &lt;- NULL # faster to match on numeric codes\n  ux &lt;- unique(product)\n  product &lt;- lapply(\n    split(product, period),\n    \\(x) match(ux, x, incomparables = NA)\n  )\n\n  v &lt;- Map(`[`, split(p * q, period), product)\n  qs &lt;- Map(`[`, split(qs, period), product)\n\n  n &lt;- nlevels(period)\n  # Make the matrices from pp. 245-246 of Balk (2008) to build the quantity\n  # index, then deflate.\n  m &lt;- vector(\"list\", n)\n  for (i in seq_along(m)) {\n    m[[i]] &lt;- vapply(\n      qs,\n      \\(qs) sum(qs * gpindex::scale_weights(v[[i]]), na.rm = na.rm),\n      numeric(1L)\n    )\n  }\n  e &lt;- diag(n)\n  r &lt;- cbind(rep.int(1, n), matrix(0, n, n - 1))\n  q &lt;- solve(do.call(rbind, m) - e + r)[1L, ]\n  v &lt;- vapply(v, sum, numeric(1L), na.rm = na.rm)\n  # Return the period-over-period index.\n  v[-1L] / v[-length(v)] / (q[-1L] / q[-length(q)])\n}\n\nAs with the WTPD index, any decent laptop can compute this index.\n\ngk_index &lt;- function(prices) {\n  fast_gk(\n    prices$price,\n    prices$quantity,\n    prices$time,\n    prices$product,\n    na.rm = TRUE\n  )\n}\n\nprice_data &lt;- make_prices(1e5, 25)\n\nbench::mark(fast_gk = gk_index(price_data))\n\n# A tibble: 1 √ó 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fast_gk       1.37s    1.37s     0.729     819MB    0.729\n\n\nIt is similarly non-trivial to make an index of this size with {IndexNumR}.\n\ngk_index2 &lt;- function(prices, window) {\n  prices[c(\"time\", \"product\")] &lt;- lapply(\n    prices[c(\"time\", \"product\")],\n    as.integer\n  )\n  res &lt;- IndexNumR::GKIndex(\n    prices,\n    \"price\",\n    \"quantity\",\n    \"time\",\n    \"product\",\n    window = window\n  )\n  as.numeric(res[-1] / res[-length(res)])\n}\n\nprice_data &lt;- make_prices(1000, 25)\n\nbench::mark(\n  fast_gk = unname(gk_index(price_data)),\n  IndexNumR = gk_index2(price_data, 25)\n)\n\n# A tibble: 2 √ó 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fast_gk      7.23ms   7.57ms   125.       8.15MB    1.99 \n2 IndexNumR      1.4s     1.4s     0.713  332.21MB    0.713"
  },
  {
    "objectID": "blog/posts/2025/multilaterals/index.html#repeat-sales",
    "href": "blog/posts/2025/multilaterals/index.html#repeat-sales",
    "title": "Making big multilateral indexes fast",
    "section": "Repeat sales",
    "text": "Repeat sales\nThe repeat-sales indexes listed by Shiller (1991) are a type of multilateral index usually found in the housing economics literature. Like the WTPD index, they come from linear models and suffer from the same large design matrices. The geometric repeat-sales index is like the WTPD index, although usually with a much longer time horizon, except that it uses the first-difference transformation to sweep out the product fixed effects instead of the within-group transformation. The arithmetic repeat-sales index is more complicated as it comes from an IV estimator and its not obvious, at least to me, how the matrices for this index can be made smaller.\nInstead of reducing the dimensionality of the repeat-sales matrices, these matrices are naturally sparse when products sell infrequently (like housing) and can be stored as sparse matrices. 1 Let‚Äôs adapt the vignette from my {rsmatrix} package to see this in action with some data for 5 million house sales over 30 years.\n\nperiods &lt;- seq(as.Date(\"2000-01-01\"), as.Date(\"2029-12-31\"), \"day\")\n\nprices &lt;- data.frame(\n  sale = sample(periods, 5e6, TRUE),\n  property = factor(sprintf(\"%06d\", sample(1:1e6, 5e6, TRUE))),\n  price = rlnorm(5e6)\n) |&gt;\n  dplyr::mutate(period = cut(sale, \"month\"))\n\nsales_pairs &lt;- rsmatrix::rs_pairs(prices$sale, prices$property)\nprices[c(\"price_prev\", \"period_prev\")] &lt;- prices[\n  sales_pairs,\n  c(\"price\", \"period\")\n]\n\nprices &lt;- prices |&gt;\n  dplyr::mutate(\n    holding_period = as.numeric(period) - as.numeric(period_prev)\n  ) |&gt;\n  dplyr::filter(holding_period &gt; 2)\n\nhead(prices)\n\n        sale property     price     period price_prev period_prev\n1 2016-05-11   627686 0.7705151 2016-05-01  0.1878927  2014-06-01\n2 2029-07-22   992771 0.2366689 2029-07-01  0.4822262  2026-10-01\n3 2011-07-17   925847 0.1712451 2011-07-01  0.5992709  2003-05-01\n4 2013-03-28   337973 0.1540939 2013-03-01  0.9106525  2002-11-01\n5 2008-04-22   295262 0.3274417 2008-04-01  0.2760829  2003-10-01\n6 2015-12-27   038822 2.2174534 2015-12-01  1.7673652  2010-11-01\n  holding_period\n1             23\n2             33\n3             98\n4            124\n5             54\n6             61\n\n\nWe can now build a generator to make the sparse repeat-sales matrices. If these were dense matrices they would consume 22.1 GB of memory. This isn‚Äôt nearly as bad as the WTPD or GK indexes, but still enough to make computing these indexes cumbersome. With sparse matrices, however, it is easy to make the repeat-sales index.\n\nmatrices &lt;- with(\n  prices,\n  rsmatrix::rs_matrix(period, period_prev, price, price_prev, sparse = TRUE)\n)\n\nZ &lt;- matrices(\"Z\")\nX &lt;- matrices(\"X\")\nY &lt;- matrices(\"Y\")\n\nbench::mark(ars = 1 / Matrix::solve(crossprod(Z, X), crossprod(Z, Y)))\n\n# A tibble: 1 √ó 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 ars           1.53s    1.53s     0.652       3MB        0"
  },
  {
    "objectID": "blog/posts/2025/multilaterals/index.html#geks",
    "href": "blog/posts/2025/multilaterals/index.html#geks",
    "title": "Making big multilateral indexes fast",
    "section": "GEKS",
    "text": "GEKS\nLet‚Äôs conclude by briefly mentioned the GEKS family of multilateral indexes. As with the others, the GEKS depends on a matrix, but it is only a \\(t \\times t\\) matrix and will require very little memory. This makes GEKS indexes somewhat more straightforward to compute than the other popular multilateral indexes.\n\n\nSession Info\n\n\n\nR version 4.5.2 (2025-10-31)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.12.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.12.0  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Toronto\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-4        jsonlite_2.0.0      dplyr_1.1.4        \n [4] compiler_4.5.2      rsmatrix_0.2.9      tidyselect_1.2.1   \n [7] Rcpp_1.1.0          yaml_2.3.10         fastmap_1.2.0      \n[10] lattice_0.22-7      R6_2.6.1            generics_0.1.4     \n[13] Formula_1.2-5       knitr_1.50          htmlwidgets_1.6.4  \n[16] tibble_3.3.0        pillar_1.11.1       rlang_1.1.6        \n[19] utf8_1.2.6          xfun_0.54           stringmagic_1.2.0  \n[22] fixest_0.13.2       cli_3.6.5           magrittr_2.0.4     \n[25] digest_0.6.38       grid_4.5.2          IndexNumR_0.6.0    \n[28] sandwich_3.1-1      gpindex_0.6.3       lifecycle_1.0.4    \n[31] nlme_3.1-168        vctrs_0.6.5         bench_1.1.4        \n[34] evaluate_1.0.5      glue_1.8.0          data.table_1.17.8  \n[37] numDeriv_2016.8-1.1 zoo_1.8-14          profmem_0.7.0      \n[40] dreamerr_1.5.0      rmarkdown_2.30      tools_4.5.2        \n[43] pkgconfig_2.0.3     htmltools_0.5.8.1"
  },
  {
    "objectID": "blog/posts/2025/multilaterals/index.html#footnotes",
    "href": "blog/posts/2025/multilaterals/index.html#footnotes",
    "title": "Making big multilateral indexes fast",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Langsrud (2024) for another example of using sparse matrices for aggregation.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2025/diversity/index.html",
    "href": "blog/posts/2025/diversity/index.html",
    "title": "Decomposing diversity indexes",
    "section": "",
    "text": "A diversity index is a way to measure the prevalence of different species in an ecosystem. Although these arise naturally in ecology, diversity indexes show up elsewhere as well. In economics, for example, we can think of diversity as related to the market concentration of firms (species) in an industry (ecosystem). What I want to show here is how we can use some of the machinery from the world of price and quantity indexes to decompose a diversity index into the contribution of each species towards overall diversity."
  },
  {
    "objectID": "blog/posts/2025/diversity/index.html#what-is-diversity-anyway",
    "href": "blog/posts/2025/diversity/index.html#what-is-diversity-anyway",
    "title": "Decomposing diversity indexes",
    "section": "What is diversity, anyway?",
    "text": "What is diversity, anyway?\nDiversity is a tricky concept to formalize. Up until writing this post, I would have said it was just the number of species in an ecosystem. In his seminal paper, Hill (1973) considers a family diversity indexes that give a measure of the effective number of species in an ecosystem‚Äîthat is, the number of species that would be present in an ecosystem if all species were equally prevalent. Mathemetically, if there are \\(n\\) species and the \\(i\\)-th species appears with probability \\(p_{i}\\), then the diversity index of order \\(\\alpha\\) is\n\\[\nN_{\\alpha}(p_{1}, \\ldots, p_{n}) = \\left(\\sum_{i=1}^{n}p_{i}^{\\alpha}\\right)^{\\frac{1}{1 - \\alpha}}.\n\\]\nDifferent values for \\(\\alpha\\) yield different indexes; for example, \\(N_{0}\\) measures diversity by the total number of species \\(n\\), also known as the richness of the ecosystem. As a function of \\(\\alpha\\), \\(N_{\\alpha}\\) is a continuously decreasing function that maps values in \\([0, \\infty)\\) onto \\([n, 1 / \\max(p_{1}, \\ldots, p_{n}))\\). We can see this with an example of an ecosystem with 10 species.\n\ndiversity_index &lt;- function(x, alpha) {\n  if (alpha != 1) {\n    sum(x^alpha)^(1 / (1 - alpha))\n  } else {\n    exp(-sum(p * log(p)))\n  }\n}\n\nset.seed(54321)\n\np &lt;- sort(gpindex::scale_weights(rlnorm(10)))\n\nhist(p, main = \"Abundance of species in an ecosystem\")\n\n\n\n\n\n\n\nalphas &lt;- seq(0, 5, 0.25)\nindex &lt;- sapply(alphas, \\(a) diversity_index(p, a))\n\nplot(\n  alphas,\n  index,\n  ylim = c(0, 10),\n  xlab = \"ùõº\",\n  ylab = \"Index\",\n  main = \"Diversity decreases with ùõº\"\n)\nabline(1 / max(p), 0, lty = \"dashed\")\n\n\n\n\n\n\n\n\nWith a bit of rearranging, we can see that \\(N_\\alpha\\) is the reciprocal of the generalized mean of \\((p_{1}, \\ldots, p_{n})\\) with these same values as weights\n\\[\nN_{\\alpha}(p_{1}, \\ldots, p_{n}) = 1 / \\left(\\sum_{i=1}^{n} p_{i}^{\\alpha - 1} p_{i}\\right)^{\\frac{1}{\\alpha - 1}}.\n\\]\nFormulating \\(N_{\\alpha}\\) as a generalized mean shows a clear link between diversity indexes and concentration indexes, as \\(N_{2}\\) is the reciprocal of the well-known Simpson index (or Herfindahl‚ÄìHirschman index if you‚Äôre an economist). (It also shows a link with measures of entropy, as \\(\\log(N_{\\alpha})\\) is a generalization of Shannon entropy.) What makes a diversity index different from a measure of concentration (or entropy) is that it expresses diversity in terms of the effective size of the ecosystem that would give rise to a particular concentration of species if species were all equally abundant. Intuitively, \\(1 / p_{i}\\) gives the effective size of the ecosystem if all species were as prevalent as species \\(i\\). Rather than considering a single species, \\(N_{a}\\) uses the average abundance across all species to arrive at a measure of diversity."
  },
  {
    "objectID": "blog/posts/2025/diversity/index.html#decomposing-diversity",
    "href": "blog/posts/2025/diversity/index.html#decomposing-diversity",
    "title": "Decomposing diversity indexes",
    "section": "Decomposing diversity",
    "text": "Decomposing diversity\nHill (1973) notes that different choices for \\(\\alpha\\) imply different sensitivities to rare versus abundant species in an ecosystem. Setting \\(\\alpha = 0\\) means that diversity depends only on the number of species, no matter how rare some may be, whereas when \\(\\alpha \\rightarrow \\infty\\) then only the most prevalent species influences diversity. We can go one step further by decomposing \\(N_{\\alpha}\\) so that it‚Äôs represented as an arithmetic mean of the effective size of each species \\(1 / p_{i}\\) and we can see the contribution of each species towards total diversity. We‚Äôll do this by borrowing some of the machinery from price and quantity indexes to derive weights \\((w_{1}, \\ldots, w_{n})\\) such that\n\\[\nN_{\\alpha}(p_{1}, \\ldots, p_{n}) = \\sum_{i=1}^{n} w_{i} / p_{i}.\n\\]\nThe core tool to do this comes from my {gpindex} package.\n\ndiversity_weights &lt;- function(x, alpha) {\n  gpindex::transmute_weights(alpha - 1, -1)(x, x)\n}\n\nWhen \\(\\alpha = 0\\), each species contributes the same amount to overall diversity.\n\ndiversity_weights(p, 0) / p\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\nIncreasing \\(\\alpha\\) to 2, the measure of diversity decreases and more weight is shifted towards more abundant species.\n\ndiversity_weights(p, 2) / p\n\n [1] 0.02204630 0.02298373 0.04003915 0.04733940 0.05467209 0.07962440\n [7] 0.10013170 0.14336078 0.48334916 1.48242257\n\n\nAs alpha becomes larger, rare species get a small weight and contribute little towards overall diversity.\n\ndiversity_weights(p, 5)\n\n [1] 4.429620e-05 4.817851e-05 1.481826e-04 2.083463e-04 2.795183e-04\n [6] 6.049503e-04 9.729488e-04 2.068354e-03 3.261787e-02 9.630074e-01\n\ndiversity_weights(p, 5) / p\n\n [1] 0.004974804 0.005190128 0.009163420 0.010897035 0.012658720 0.018811300\n [7] 0.024058228 0.035722328 0.167085953 1.608432485\n\n\nThis gives a different way to view the prevalence of a species, not just by their abundance but by how their abundance contributes towards overall diversity in an ecosystem."
  },
  {
    "objectID": "blog/posts/2025/inflation/index.html",
    "href": "blog/posts/2025/inflation/index.html",
    "title": "What causes inflation?",
    "section": "",
    "text": "What causes inflation in prices over time? Increases in the money supply, of course. Although there are many reasons for increasing prices over time, money supply is one of the few things that can do so and grow without bound. Case closed, right? Not quite.\nIn practice we can‚Äôt perfectly observe how prices change over time and instead we must measure inflation with an index number. Ignoring all the details that arise in practice, the goal is usually to construct a chained Laspeyres or Fisher index. There‚Äôs a nice disclaimer in the CPI manual about potential issues with these formulas when prices bounce or oscillate, rather than grow monotonically (IMF et al. 2025, 24). What I want to show here is that this can be a source of perpetual inflation. This means that, for example, increases and decreases in market concentration over time, which would result in fluctuating prices, can result a price index measuring inflation, even when prices don‚Äôt cumulatively change over time."
  },
  {
    "objectID": "blog/posts/2025/inflation/index.html#setup",
    "href": "blog/posts/2025/inflation/index.html#setup",
    "title": "What causes inflation?",
    "section": "Setup",
    "text": "Setup\nLet‚Äôs start with a simple two-commodity model where a representative consumer has Cobb-Douglas preferences. We‚Äôll make a small class and mixin to make a Laspeyres and Fisher index given prices based on these preferences.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass IndexMixin:\n    def laspeyres(self, p1: np.ndarray, p0: np.ndarray) -&gt; float:\n        q0 = self.demand(p0)\n        return np.sum(p1 * q0) / np.sum(p0 * q0)\n\n    def paasche(self, p1: np.ndarray, p0: np.ndarray) -&gt; float:\n        q1 = self.demand(p1)\n        return np.sum(p1 * q1) / np.sum(p0 * q1)\n\n    def fisher(self, p1: np.ndarray, p0: np.ndarray) -&gt; float:\n        laspeyres = self.laspeyres(p1, p0)\n        paasche = self.paasche(p1, p0)\n        return np.sqrt(laspeyres * paasche)\n\n\nclass CobbDouglas(IndexMixin):\n    def __init__(self, alpha: np.ndarray, m: float):\n        self._alpha = np.asarray(alpha, dtype=np.float64)\n        self._alpha /= np.sum(self._alpha)\n        self._m = float(m)\n\n    def demand(self, p: np.ndarray) -&gt; np.ndarray:\n        if len(p) != len(self._alpha):\n            raise ValueError(f\"must supply prices for {len(self._alpha)} products\")\n        return self._alpha * self._m / p\n\nWe‚Äôll also make a function to oscillate prices. The idea is to form a chain of index values that starts with both products having a price of [1, 1], switches prices between [1, 1.1] and [1.1, 1] some number of times, and ends with prices back at [1, 1]. As a chained index is just the cumulative product of these index values, we can decompose the chained index into the product of the first and last change in price, and the product of the oscillations. What‚Äôs necessary to drive inflation is that the product of the oscillations is greater than 1.\nWe‚Äôll also make a second function that does a more complex oscillation that we‚Äôll use later.\n\ndef bounce(index) -&gt; tuple[np.float64]:\n    initial = [1, 1]\n    p1 = [1, 1.1]\n    p2 = [1.1, 1]\n    i = index(p1, initial) * index(initial, p1)\n    b = index(p2, p1) * index(p1, p2)\n    return i, b\n\n\ndef bounce2(index) -&gt; tuple[np.float64]:\n    initial = [1, 1]\n    p1 = [1, 1.1]\n    p2 = [1.2, 1]\n    p3 = [1.1, 1]\n    i = index(p1, initial) * index(initial, p1)\n    b = index(p2, p1) * index(p3, p2) * index(p1, p3)\n    return i, b"
  },
  {
    "objectID": "blog/posts/2025/inflation/index.html#inflation-due-to-measurement",
    "href": "blog/posts/2025/inflation/index.html#inflation-due-to-measurement",
    "title": "What causes inflation?",
    "section": "Inflation due to measurement",
    "text": "Inflation due to measurement\nLet‚Äôs start by generating some preferences and giving our representative consumer a fixed income, then simulating how the chained Laspeyres index evolves as prices bounce around.\n\nu = CobbDouglas([1, 2], 100)\n\nres = bounce(u.laspeyres)\n\noscillate = [res[0] * res[1]**n for n in range(20)]\n\nfig, ax = plt.subplots()\nax.plot(oscillate)\nplt.xticks(range(0, 20, 2))\nplt.show()\n\n\n\n\n\n\n\n\nHere we see that, despite prices starting and ending at the same level, the chained Laspeyres index shows that prices have increased over time. This happens because each oscillation registers an increase in prices (about 0.8%), and these compound to show that prices are increasing over time. Note that the culprit here is the well-known substitution bias in the Laspeyres index; if there was no substitution bias then the index would be transitive with Cobb-Douglas preferences and (correctly) show no change in prices over time. Instead, this happens with Leontief preferences.\n\nclass Leontief(IndexMixin):\n    def __init__(self, m: float):\n        self._m = float(m)\n\n    def demand(self, p: np.ndarray) -&gt; np.ndarray:\n        return np.repeat(self._m / np.sum(p), len(p))\n\nbounce(Leontief(100).laspeyres)\n\n(np.float64(1.0), np.float64(1.0))\n\n\nBoth components of the chained index‚Äîthe first and last change in price and the part due to oscillations‚Äîshow no change in price.\nWe can get the same behavior from the Fisher index, albeit with a more complex form of oscillation. (Using the simple oscillation would show no change in prices over time because the Fisher index satisfies the time-reversal property.)\n\nres = bounce2(u.fisher)\n\noscillate = [res[0] * res[1] ** n for n in range(20)]\n\nfig, ax = plt.subplots()\nax.plot(oscillate)\nplt.xticks(range(0, 20, 2))\nplt.show()\n\n\n\n\n\n\n\n\nEach oscillation registers a 0.02% increase in prices. Note that this behavior from the Fisher index is sensitive to the parameters in the Cobb-Douglas utility function and it‚Äôs also possible to get that prices decreases over time.\n\nu = CobbDouglas([2, 1], 100)\nres = bounce2(u.fisher)\nprint(res)\n\n(np.float64(1.0), np.float64(0.9998308882582081))\n\n\nOverall, this example shows that more than just money supply can drive our measurement of inflation, even if increasing money supply is the only true cause of inflation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Price statistics and index numbers\n\n\nR programming and data science\n\n\n\n\nIndustrial organization\n\n\nEnvironmental economics"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About me",
    "section": "",
    "text": "Price statistics and index numbers\n\n\nR programming and data science\n\n\n\n\nIndustrial organization\n\n\nEnvironmental economics"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\nDegrees\n\nPhD Economics, University of Ottawa, 2013-2017.\nB.SocSc Economics, University of Ottawa, 2009-2012.\n\n\n\nOther education\n\nEuropean Summer School in Environmental Economics, Venice International University, 2015.\nVisiting Scholar, University of Michigan, 2015."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About me",
    "section": "Work experience",
    "text": "Work experience\n\nEmployment\n\nPrincipal researcher, Statistics Canada, 2024‚Äìpresent\nData scientist, Government of Canada, 2022‚Äì2024.\nEconomist, Statistics Canada, 2017‚Äì2022.\nPart-time professor of economics, University of Ottawa, 2016‚Äì2018.\n\n\n\nTeaching\n\nA Course on Price Indices, Statistics Canada, 2019‚Äì2022.\nDigital Academy (lecture on R), Canada School of Public Service, Spring and Fall 2019.\nResearch Seminar in Economics of the Environment, University of Ottawa, Fall 2016 and Winter 2018."
  },
  {
    "objectID": "about.html#awards-and-prizes",
    "href": "about.html#awards-and-prizes",
    "title": "About me",
    "section": "Awards and Prizes",
    "text": "Awards and Prizes\n\nEconomic Statistics Field Shining Star Award, Statistics Canada, 2025.\nOutstanding Contribution Award, Statistics Canada, 2020.\nYouth Leadership Award, Statistics Canada, 2018.\nPierre Laberge prize for outstanding doctoral thesis, University of Ottawa, 2018.\nJoseph-Armande Bombardier Canadian Graduate Scholarship, SSHRC, 2015."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Most price indexes are made with a two-step procedure, where period-over-period elementary indexes are first calculated for a collection of elementary aggregates at each point in time, and then aggregated according to a price index aggregation structure. These indexes can then be chained together to form a time series that gives the evolution of prices with respect to a fixed base period. This package contains a collection of functions that revolve around this work flow, making it easy to build standard price indexes, and implement the methods described by Balk (2008), von der Lippe (2007), and the CPI manual (2020) / PPI manual (2004) for bilateral price indexes.\n\n\n\n\n\n\n    \nSequential Poisson sampling is a variation of Poisson sampling for drawing probability-proportional-to-size samples with a given number of units, and is commonly used for price-index surveys. This package gives functions to draw stratified sequential Poisson samples according to the method by Ohlsson (1998), as well as other order sample designs by Ros√©n (1997), and generate approximate bootstrap replicate weights according to the generalized bootstrap method by Beaumont and Patak (2012).\n\n\n\n\n\n\n   \nTools to build and work with bilateral generalized-mean price indexes (and by extension quantity indexes), and indexes composed of generalized-mean indexes (e.g., superlative quadratic-mean indexes, GEKS). Covers the core mathematical machinery for making bilateral price indexes, computing price relatives, detecting outliers, and decomposing indexes, with wrappers for all common (and many uncommon) index-number formulas. Implements and extends many of the methods in Balk (2008), von der Lippe (2007), and the CPI manual (2020).\n\n\n\n\n\n\n  \nCalculate the matrices in Shiller (1991) that serve as the foundation for many repeat-sales price indexes."
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "Software",
    "section": "",
    "text": "Most price indexes are made with a two-step procedure, where period-over-period elementary indexes are first calculated for a collection of elementary aggregates at each point in time, and then aggregated according to a price index aggregation structure. These indexes can then be chained together to form a time series that gives the evolution of prices with respect to a fixed base period. This package contains a collection of functions that revolve around this work flow, making it easy to build standard price indexes, and implement the methods described by Balk (2008), von der Lippe (2007), and the CPI manual (2020) / PPI manual (2004) for bilateral price indexes.\n\n\n\n\n\n\n    \nSequential Poisson sampling is a variation of Poisson sampling for drawing probability-proportional-to-size samples with a given number of units, and is commonly used for price-index surveys. This package gives functions to draw stratified sequential Poisson samples according to the method by Ohlsson (1998), as well as other order sample designs by Ros√©n (1997), and generate approximate bootstrap replicate weights according to the generalized bootstrap method by Beaumont and Patak (2012).\n\n\n\n\n\n\n   \nTools to build and work with bilateral generalized-mean price indexes (and by extension quantity indexes), and indexes composed of generalized-mean indexes (e.g., superlative quadratic-mean indexes, GEKS). Covers the core mathematical machinery for making bilateral price indexes, computing price relatives, detecting outliers, and decomposing indexes, with wrappers for all common (and many uncommon) index-number formulas. Implements and extends many of the methods in Balk (2008), von der Lippe (2007), and the CPI manual (2020).\n\n\n\n\n\n\n  \nCalculate the matrices in Shiller (1991) that serve as the foundation for many repeat-sales price indexes."
  },
  {
    "objectID": "software.html#python-packages",
    "href": "software.html#python-packages",
    "title": "Software",
    "section": "Python packages",
    "text": "Python packages\n\npysps\n\n\n\n \nSequential Poisson sampling is a variation of Poisson sampling for drawing probability-proportional-to-size samples with a given number of units, and is commonly used for price-index surveys. This package is a Python implementation of the {sps} R package."
  },
  {
    "objectID": "software.html#contributed-to",
    "href": "software.html#contributed-to",
    "title": "Software",
    "section": "Contributed to",
    "text": "Contributed to\nI‚Äôve made several (usually) small contributions to various open source projects.\n\n\n\n{accumulate}\n\n\n{bootstrapFP}\n\n\n{concstats}\n\n\n\n\nDVC\n\n\n{goodpractice}\n\n\n{gseries}\n\n\n\n\n{httr2}\n\n\n{lintr}\n\n\nR (base and stats)\n\n\n\n\n{sampling}\n\n\n{tabulapdf}\n\n\n{tempodisco}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve Martin",
    "section": "",
    "text": "Hi there, I‚Äôm Steve üëã. Welcome to my personal website.\nI‚Äôm a principal researcher at Statistics Canada where I use economics and data science to measure the economy.\nI work on several side projects in my spare time building open source statistical software, particularly in the area of price statistics. Checkout software for more information.\nPrior to working at Statistics Canada, I completed my PhD in economics where I examined firms‚Äô corporate social responsibility and its implication for the efficient provision of public goods. Checkout research and publications for more information."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research and Publications",
    "section": "",
    "text": "Martin, S. (2024). piar: Price Index Aggregation in R. Journal of Open Source Software, 9(101): 6781.\nKirby-McGregor, M. and Martin, S. (2019). An R package for calculating repeat-sale price indices. Romanian Statistical Review, 3/2019: 17-33.\n\n\n\n\n\nMartin, S. (2021). A note on general decompositions for price indexes. Prices Analytical Series. Statistics Canada Catalogue 62F0014M.\nMartin, S. (2020). Gearing up to stay open: Trends in businesses‚Äô needs for personal protective equipment since July. Statcan COVID-19: Data Insights for a Better Canada. Statistics Canada Catalogue 45-28-0001.\nMartin, S. (2020). Gearing up to restart: Businesses‚Äô need for personal protective equipment. Statcan COVID-19: Data Insights for a Better Canada. Statistics Canada Catalogue 45-28-0001.\nMartin, S. and Rouleau, B. (2020). An exploration of work, learning, and work-integrated learning in Canada using the Longitudinal and International Study of Adults. LISA Research Paper Series. Statistics Canada Catalogue 89-648-X.\nMartin, S. (2019). Methodology of the Residential Property Price Index (RPPI). Prices Analytical Series. Statistics Canada Catalogue 62F0014M.\nMartin, S. (2018). The association between job flexibility and job satisfaction. Insights on Canadian Society. Statistics Canada Catalogue 75-006-X.\nMartin, S. (2018). In sickness and in health: The association between health and household income. LISA Research Paper Series. Statistics Canada Catalogue 89-648-X.\n\n\n\n\n\nHeyes, A., Kapur, S., Kennedy, P., Martin, S., and Maxwell, J. (2020). But What Does it Mean? Competition between Products Carrying Alternative Green Labels when Consumers are Active Acquirers of Information. Journal of the Association of Environmental and Resource Economists, 7(2): 243-277.\nMartin, S. (2019). Moral management in competitive markets. Journal of Economics and Management Strategy, 28(3): 541‚Äì560.\nHeyes, A. and Martin, S. (2018). Inefficient social labels: Strategic proliferation and fragmentation in the market for certification. Journal of Economics and Management Strategy, 27(2): 206‚Äì220.\nHeyes, A., Lyon, T. P., and Martin, S. (2018). Salience games: Private politics when public attention is limited. Journal of Environmental Economics and Management, 88: 396‚Äì410.\nMartin, S. and Rivers, N. (2018). Information provision, market incentives, and household electricity consumption: Evidence from a large-scale field deployment. Journal of the Association of Environmental and Resource Economists, 5(1): 207‚Äì231.\nHeyes, A. and Martin, S. (2017). Social labeling by competing NGOs: A model with multiple issues and entry. Management Science, 63(6): 1800‚Äì1813.\nHeyes, A. and Martin, S. (2016). Fuzzy products. International Journal of Industrial Organization, 45: 1‚Äì9.\nHeyes, A. and Martin, S. (2015). NGO mission design. Journal of Economic Behavior & Organization, 119: 197‚Äì210.\n\n\n\n\n\nHa, X. and Martin, S. (2025). Modernizing Official Statistics: Transition to Open-Source Systems for Production of Producer Price Indexes. Voorburg Group on Service Statistics.\nBrown, C. and Martin, S. (2025). Interim Report: Survey of CPI Production Systems. UNECE Meeting of the Group of Experts on Consumer Price Indices.\nBurnett-Isaacs, K. and Martin, S. (2019). Unifying Approach to Statistics Canada¬¥s Residential Property Price Index. UNECE Ottawa Group on Price Indices.\nBurnett-Isaacs, K. and Martin, S. (2019). Constructing a Resale Residential Property Price Index for Vancouver using Repeat Sales. UNECE Ottawa Group on Price Indices."
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "Research and Publications",
    "section": "",
    "text": "Martin, S. (2024). piar: Price Index Aggregation in R. Journal of Open Source Software, 9(101): 6781.\nKirby-McGregor, M. and Martin, S. (2019). An R package for calculating repeat-sale price indices. Romanian Statistical Review, 3/2019: 17-33.\n\n\n\n\n\nMartin, S. (2021). A note on general decompositions for price indexes. Prices Analytical Series. Statistics Canada Catalogue 62F0014M.\nMartin, S. (2020). Gearing up to stay open: Trends in businesses‚Äô needs for personal protective equipment since July. Statcan COVID-19: Data Insights for a Better Canada. Statistics Canada Catalogue 45-28-0001.\nMartin, S. (2020). Gearing up to restart: Businesses‚Äô need for personal protective equipment. Statcan COVID-19: Data Insights for a Better Canada. Statistics Canada Catalogue 45-28-0001.\nMartin, S. and Rouleau, B. (2020). An exploration of work, learning, and work-integrated learning in Canada using the Longitudinal and International Study of Adults. LISA Research Paper Series. Statistics Canada Catalogue 89-648-X.\nMartin, S. (2019). Methodology of the Residential Property Price Index (RPPI). Prices Analytical Series. Statistics Canada Catalogue 62F0014M.\nMartin, S. (2018). The association between job flexibility and job satisfaction. Insights on Canadian Society. Statistics Canada Catalogue 75-006-X.\nMartin, S. (2018). In sickness and in health: The association between health and household income. LISA Research Paper Series. Statistics Canada Catalogue 89-648-X.\n\n\n\n\n\nHeyes, A., Kapur, S., Kennedy, P., Martin, S., and Maxwell, J. (2020). But What Does it Mean? Competition between Products Carrying Alternative Green Labels when Consumers are Active Acquirers of Information. Journal of the Association of Environmental and Resource Economists, 7(2): 243-277.\nMartin, S. (2019). Moral management in competitive markets. Journal of Economics and Management Strategy, 28(3): 541‚Äì560.\nHeyes, A. and Martin, S. (2018). Inefficient social labels: Strategic proliferation and fragmentation in the market for certification. Journal of Economics and Management Strategy, 27(2): 206‚Äì220.\nHeyes, A., Lyon, T. P., and Martin, S. (2018). Salience games: Private politics when public attention is limited. Journal of Environmental Economics and Management, 88: 396‚Äì410.\nMartin, S. and Rivers, N. (2018). Information provision, market incentives, and household electricity consumption: Evidence from a large-scale field deployment. Journal of the Association of Environmental and Resource Economists, 5(1): 207‚Äì231.\nHeyes, A. and Martin, S. (2017). Social labeling by competing NGOs: A model with multiple issues and entry. Management Science, 63(6): 1800‚Äì1813.\nHeyes, A. and Martin, S. (2016). Fuzzy products. International Journal of Industrial Organization, 45: 1‚Äì9.\nHeyes, A. and Martin, S. (2015). NGO mission design. Journal of Economic Behavior & Organization, 119: 197‚Äì210.\n\n\n\n\n\nHa, X. and Martin, S. (2025). Modernizing Official Statistics: Transition to Open-Source Systems for Production of Producer Price Indexes. Voorburg Group on Service Statistics.\nBrown, C. and Martin, S. (2025). Interim Report: Survey of CPI Production Systems. UNECE Meeting of the Group of Experts on Consumer Price Indices.\nBurnett-Isaacs, K. and Martin, S. (2019). Unifying Approach to Statistics Canada¬¥s Residential Property Price Index. UNECE Ottawa Group on Price Indices.\nBurnett-Isaacs, K. and Martin, S. (2019). Constructing a Resale Residential Property Price Index for Vancouver using Repeat Sales. UNECE Ottawa Group on Price Indices."
  },
  {
    "objectID": "publications.html#other-research-activities",
    "href": "publications.html#other-research-activities",
    "title": "Research and Publications",
    "section": "Other research activities",
    "text": "Other research activities\n\nConference presentations\n\nStatistics Canada‚Äôs Price Measurement Advisory Committee (Ottawa, 2019 and 2025).\n52nd Annual Conference of the Canadian Economics Association (Montreal, 2018).\nAnnual meeting of the Canadian Public Economics Group (Montreal, 2016).\n50th Annual Conference of the Canadian Economics Association (Ottawa, 2016).\n4th Canadian Early Career Workshop in Environmental Economics (Ottawa, 2016).\n14th Annual International Industrial Organization Conference (Philadelphia, 2016).\nEAERE-FEEM-VIU European Summer School (Venice, 2015).\n5th World Congress of Environmental and Resource Economists (Istanbul, 2014).\n12th Annual International Industrial Organization Conference (Chicago, 2014).\n\n\n\nRefereed for\n\n\n\nCanadian Journal of Economics\n\n\nEnvironmental and Resource Economics\n\n\n\n\nJournal of the Association of Environmental and Resource Economists\n\n\nJournal of Economics and Management Strategy\n\n\n\n\nJournal of Environmental Economics and Management\n\n\nJournal of Open Source Software"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Inflationary Tendencies",
    "section": "",
    "text": "Welcome to my blog. I write about topics in economics, statistics, and open source software (mostly so I don‚Äôt forget about it). All views expressed here are my own and do not necessarily reflect those of my employer or any affiliated organizations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking big multilateral indexes fast\n\n\n\nPerformance\n\nIndex numbers\n\nR\n\n\n\nMultilateral price indexes that use large volumes of transaction data can be challenging to compute. I show some tricks to calculate big indexes fast.\n\n\n\n\n\nNov 24, 2025\n\n\nSteve Martin\n\n\n\n\n\n\n\n\n\n\n\n\nHow does the difference between the arithmetic and geometric mean depend on variance?\n\n\n\nInequalities\n\nIndex numbers\n\nR\n\n\n\nIt‚Äôs well known that the arithmetic mean is larger than the geometric mean, but how does the difference between these means relate to variance?\n\n\n\n\n\nNov 12, 2025\n\n\nSteve Martin\n\n\n\n\n\n\n\n\n\n\n\n\nHedonic imputation with one regression, not two\n\n\n\nIndex numbers\n\nEconometrics\n\nR\n\n\n\nHedonic imputation is a standard method to compare prices over time when the composition of products also changes. In all cases I‚Äôve seen, the method is implemented by using the fitted values from two regression models. I show how it can be done with one regression.\n\n\n\n\n\nOct 27, 2025\n\n\nSteve Martin\n\n\n\n\n\n\n\n\n\n\n\n\nWhat causes inflation?\n\n\n\nIndex numbers\n\nPython\n\n\n\nLong-term inflation requires something that increase prices and grow without bound (i.e., money supply). I show that the way that inflation is measured can itself be a source of inflation.\n\n\n\n\n\nOct 22, 2025\n\n\nSteve Martin\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposing diversity indexes\n\n\n\nIndex numbers\n\nR\n\n\n\nA diversity index is a way to measure the prevalence of different species in an ecosystem. I show how methods from the price-index literature can be used to decompose a diversity index into the contribution of each species towards overall diversity.\n\n\n\n\n\nOct 19, 2025\n\n\nSteve Martin\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025/variance-inequality/index.html",
    "href": "blog/posts/2025/variance-inequality/index.html",
    "title": "How does the difference between the arithmetic and geometric mean depend on variance?",
    "section": "",
    "text": "That the arithmetic mean of a set of number is larger than the geometric mean is a foundational result in the theory of inequalities. This is a useful result for index numbers in particular as it gives that, for the same data, an index based on an arithmetic mean will always be larger than the corresponding index based on the geometric mean.\nThe latest edition of the CPI manual says the following about the difference between the arithmetic and geometric mean and how it relates to variance (IMF et al. 2020, 2)\nThis seems like a great result that gives a way to easily think about the relationship between an index that uses an arithmetic mean to aggregate price relatives (Carli index) and one that uses a geometric mean (Jevons index). 1 Unfortunately it‚Äôs not true‚Äîsee Lord (2002) for a neat way to generate counter examples. 2\nThe issue is that, for a vector of positive numbers \\(x\\in\\mathbf{R}_{n}^{++}\\), the difference between the arithmetic mean \\(\\mathfrak{A}(x)\\) and geometric mean \\(\\mathfrak{G}(x)\\) is only bounded by the variance (Bullen 2003, 156)\n\\[\n\\frac{\\text{var}(x)}{2 \\max(x)} \\leq \\mathfrak{A}(x) - \\mathfrak{G}(x) \\leq \\frac{\\text{var}(x)}{2 \\min(x)}.\n\\]\nFor two vectors \\(x\\) and \\(y\\), we have that \\(\\mathfrak{A}(y) - \\mathfrak{G}(y) &gt; \\mathfrak{A}(x) - \\mathfrak{G}(x)\\) if\n\\[\n\\text{var}(y) &gt; \\frac{\\max(y)}{\\min(x)}\\text{var}(x)\n\\tag{1}\\]\nand the opposite if\n\\[\n\\text{var}(y) &lt; \\frac{\\min(y)}{\\max(x)}\\text{var}(x)\n\\tag{2}\\]\nIf \\(\\max(y) &gt; \\min(x)\\) (what we should expect) then the difference between the arithmetic and geometric mean increases with a sufficiently large increase in variance. Similarly, if \\(\\min(y) &lt; \\max(x)\\) then the difference between the arithmetic and geometric mean decreases with a sufficiently large decrease in variance. If the support of the distribution is sufficiently narrow then the difference between the arithmetic and geometric mean will follow closely with the variance, and this is similar to the argument that generates the claim in the CPI manual (IMF et al. 2025, 131).\nWhat I want to understand is how the difference between the arithmetic and geometric mean changes for smaller changes in variance without placing restrictions on the support of the distribution. Let‚Äôs consider drawing a collection of samples from a fixed probability distribution to see how the difference between the arithmetic and geometric mean relates to variance among samples from that distribution. We‚Äôll make a few helper functions for this.\nvar_ineq &lt;- function(x) {\n  m &lt;- mean(x)\n  c(\n    var = mean((x - m)^2),\n    diff = m - exp(mean(log(x))),\n    min = min(x),\n    max = max(x)\n  )\n}\n\nsimulate_var_ineq &lt;- function(n, sim, dist) {\n  res &lt;- apply(matrix(dist(n * sim), nrow = n), 2, var_ineq)\n  res[, order(res[\"var\", ])]\n}\n\nprob_decrease &lt;- function(x) {\n  apply(\n    x,\n    2,\n    \\(z) {\n      cond &lt;- x[\"var\", ] &lt;= x[\"max\", ] / z[\"min\"] * z[\"var\"] |\n        x[\"var\", ] &gt;= x[\"min\", ] / z[\"max\"] * z[\"var\"]\n      x &lt;- x[, cond]\n      decrease &lt;- x[\"diff\", ] &lt; z[\"diff\"] & x[\"var\", ] &gt; z[\"var\"]\n      increase &lt;- x[\"diff\", ] &gt; z[\"diff\"] & x[\"var\", ] &lt; z[\"var\"]\n\n      mean(decrease | increase) * 100\n    }\n  )\n}\nThe scatter plot of the difference between the arithmetic and geometric mean and variance for a sample of size 10 shows that, although the difference tends to increase with variance, there are several instances where the opposite occurs.\nset.seed(15243)\n\nsim &lt;- simulate_var_ineq(10, 100, \\(n) rlnorm(n, sdlog = 0.25))\n\nplot(t(sim), ylab = \"arithmetic mean - geometric mean\", xlab = \"variance\")\nsymbols(0.13, 0.05, circles = 0.015, inches = FALSE, add = TRUE, lty = 2)\nsymbols(0.065, 0.03, circles = 0.01, inches = FALSE, add = TRUE, lty = 2)\nTo better understand this, let‚Äôs simulate the probability that the difference between the arithmetic and geometric mean decreases with variance, conditional on Equation¬†1 and Equation¬†2 not holding.\nres &lt;- simulate_var_ineq(10, 5000, \\(n) rlnorm(n, sdlog = 0.25)) |&gt;\n  prob_decrease()\n\nsummary(res)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   4.400   6.340   6.562   8.020  35.160\nOverall, the median of the conditional probability that the difference between the arithmetic and geometric mean decreases with variance is about 6.3%. This is a fairly small probability for the difference between the means to decrease when variance increases, although it depends on the underlying population from which samples are drawn. Drawing samples from a distribution with a wider support makes it more likely to see the difference between the arithmetic and geometric mean decreases with variance.\nres &lt;- simulate_var_ineq(10, 5000, \\(n) runif(n, 0.25, 4)) |&gt;\n  prob_decrease()\n\nsummary(res)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.020   8.515  13.400  13.579  17.100  50.580\nAs the support of the distribution shrinks, the difference between the arithmetic and geometric mean follows the variance more closely and the probabiltity that the difference in means decreases as variance increases becomes small.\nres &lt;- simulate_var_ineq(10, 5000, \\(n) runif(n, 0.9, 1 / 0.9)) |&gt;\n  prob_decrease()\n\nsummary(res)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.800   1.140   1.194   1.500   4.560"
  },
  {
    "objectID": "blog/posts/2025/variance-inequality/index.html#footnotes",
    "href": "blog/posts/2025/variance-inequality/index.html#footnotes",
    "title": "How does the difference between the arithmetic and geometric mean depend on variance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe CPI manual is more precise on page 179 and notes that the difference between the arithmetic and geometric mean tends to increase with variance.‚Ü©Ô∏é\nThis can also apply to the difference between the arithmetic mean and harmonic mean, where it is often suggested that the difference between these means increases with variance (IMF et al. 2025, 131).‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2025/hedonics/index.html",
    "href": "blog/posts/2025/hedonics/index.html",
    "title": "Hedonic imputation with one regression, not two",
    "section": "",
    "text": "Comparing housing prices over time is difficult because of all the heterogeneity in housing that can both affect prices and vary over time. The typical solution to this problem is to use a hedonic price index that attempts to model the relationship between housing characteristics and price, and uses this to control the change in these characteristics over time.\nAlthough there are several approaches to building a hedonic price index, the double imputation approach is usually seen as the best. Every presentation of this method that I have seen calculates the index by fitting two regression models and combining the fitted values from these models in a geometric index (e.g., Aizcorbe 2014; ILO et al. 2013; IMF 2020). What I want to show here is that the double imputation index can be recovered from the coefficient in a linear regression. This makes it simpler to calculate the index and provides an easy strategy to estimate conventional standard errors."
  },
  {
    "objectID": "blog/posts/2025/hedonics/index.html#turning-two-regressions-into-one",
    "href": "blog/posts/2025/hedonics/index.html#turning-two-regressions-into-one",
    "title": "Hedonic imputation with one regression, not two",
    "section": "Turning two regressions into one",
    "text": "Turning two regressions into one\nThe hedonic imputation model starts with two linear models, one for transactions in period 0,\n\\[\n\\log(p_{i0}) = \\alpha_{0} + x_{i0} \\beta_{0} + u_{i0},\n\\tag{1}\\]\nand one for transactions in period 1,\n\\[\n\\log(p_{i1}) = \\alpha_{1} + x_{i1} \\beta_{1} + e_{i1},\n\\tag{2}\\]\nwhere \\(x_{it}\\) is a vector of characteristics that confound a change in price over time with a change in the composition of housing.\nThe Laspeyres hedonic imputation index, \\(I\\), compares the predicted prices from Equation¬†2 against those from Equation¬†1 over the distribution of characteristics in period 0\n\\[\n\\log(I) = \\alpha_{1} - \\alpha_{0} + \\bar{x}_{0} (\\beta_{1} - \\beta_{0}),\n\\tag{3}\\]\nwhere \\(\\bar{x}_{0}\\) is the vector of average period 0 characteristics. This is equation 5.19 in ILO et al. (2013).\nEquation¬†1 and Equation¬†2 can be nested to get a single linear model. Subtracting \\(\\bar{x}_{0}\\) from the interaction term then gives Equation¬†3 as a coefficient on a time dummy variable\n\\[\\begin{align}\n\\log(p_{it}) &= \\alpha_{0} + x_{it} \\beta_{0} + t (\\alpha_{1} - \\alpha_{0} + x_{it} (\\beta_{1} - \\beta_{0})) + u_{it} + t(e_{it} - u_{it})\\\\\n\n&= \\alpha_{0} + t \\log(I) + x_{it} \\beta_{0} + t (x_{it} - \\bar{x}_{0}) (\\beta_{1} - \\beta_{0}) + \\varepsilon_{it},\n\\end{align}\\]\nwhere \\(\\varepsilon_{it} = u_{it} + t(e_{it} - u_{it})\\). The Paasche hedonic imputation index follows along the same lines, replacing \\(\\bar{x}_{0}\\) with \\(\\bar{x}_{1}\\). The Fisher hedonic imputation index combines these two, which is the same as replacing \\(\\bar{x}_{0}\\) with \\(\\bar{x}_{0} / 2 + \\bar{x}_{1} / 2\\). Note that this reduces to the classic time-dummy hedonic index when \\(\\beta_{1} = \\beta_{0}\\).1"
  },
  {
    "objectID": "blog/posts/2025/hedonics/index.html#an-example",
    "href": "blog/posts/2025/hedonics/index.html#an-example",
    "title": "Hedonic imputation with one regression, not two",
    "section": "An example",
    "text": "An example\nLet‚Äôs go through an example to see this in action. As the goal is just to give an example, I‚Äôll make up some typical-looking transaction data for property sales with a few characteristics over two periods.\n\nset.seed(15243)\n\nsales &lt;- data.frame(\n  period = rep(0:1, c(120, 180)),\n  price = rlnorm(300),\n  area = runif(300, 800, 2500),\n  age = sample(1:80, 300, replace = TRUE),\n  amenity_score = runif(300)\n)\n\nhead(sales)\n\n  period     price      area age amenity_score\n1      0 1.8931980 2415.7592  70    0.01447159\n2      0 0.9737266 2244.8349   5    0.59602963\n3      0 0.9860096 1937.4438  80    0.39965375\n4      0 1.4830738  989.7379  66    0.16038727\n5      0 0.3651522 2324.3656  50    0.82509594\n6      0 0.7955844 1930.3795  12    0.38836734\n\n\nWe‚Äôll work with the following basic model relating price with characteristics in each period\n\\[\n\\log(\\text{price}_{it}) = \\alpha_{t} + \\beta_{1t}\\text{age}_{it} + \\beta_{2t}\\text{area}_{it} + \\beta_{i3}\\text{area}_{it}^2 + \\beta_{i4}\\text{neighbourhoodscore}_{it} + \\varepsilon_{it}.\n\\]\nThe two-regression approach for making the Laspeyres-imputation index fits two separate models for each time period and compares the average ratio of predicted prices over the distribution of housing characteristics for sales in period 0.\n\nsales2 &lt;- split(sales, ~period)\n\nmdl1 &lt;- lm(log(price) ~ age + area + I(area^2) + amenity_score, sales2[[1]])\n\nmdl2 &lt;- lm(log(price) ~ age + area + I(area^2) + amenity_score, sales2[[2]])\n\nlaspeyres &lt;- exp(mean(predict(mdl2, sales2[[1]]) - predict(mdl1, sales2[[1]])))\n\nlaspeyres\n\n[1] 0.9367763\n\n\nThe Paasche-imputation index does the same thing, just over the distribution of characteristics for sales in period 1, and the Fisher-imputation index combines the Laspeyres and Paasche indexes.\n\npaasche &lt;- exp(mean(predict(mdl2, sales2[[2]]) - predict(mdl1, sales2[[2]])))\n\npaasche\n\n[1] 0.9376688\n\nfisher &lt;- sqrt(laspeyres * paasche)\n\nfisher\n\n[1] 0.9372224\n\n\nTo make these indexes with one regression we need to remove the appropriate mean from the characteristics variables and interact them with the time dummy in the linear model. In each case the coefficient on the time dummy variable gives the same index as using two regressions. For simplicity, I‚Äôll just show the case of the Fisher index.\n\ndm &lt;- function(x) {\n  m0 &lt;- mean(x[eval(substitute(period == 0), parent.frame())])\n  m1 &lt;- mean(x[eval(substitute(period == 1), parent.frame())])\n  x - mean(c(m0, m1))\n}\n\nmdl_fisher &lt;- lm(\n  log(price) ~ period +\n    age +\n    area +\n    I(area^2) +\n    amenity_score +\n    period:(dm(age) + dm(area) + dm(area^2) + dm(amenity_score)),\n  sales\n)\n\nall.equal(exp(coef(mdl_fisher)[\"period\"]), fisher, check.attributes = FALSE)\n\n[1] TRUE\n\n\nRepresenting the Fisher index as the coefficient in a linear model also makes it easy to get the (delta method) standard error for the index. 2\n\nexp(coef(mdl_fisher)[\"period\"]) * sqrt(sandwich::vcovHC(mdl_fisher)[2, 2])\n\n   period \n0.1009944 \n\n\nAdding weights to both the regression model and the index-number formulas is a simple extension. Continuing with the above example, suppose we want to weight by house price so that more expensive houses received a larger weight.\n\nmdl1 &lt;- lm(\n  log(price) ~ age + area + I(area^2) + amenity_score,\n  sales2[[1]],\n  weights = price\n)\n\nmdl2 &lt;- lm(\n  log(price) ~ age + area + I(area^2) + amenity_score,\n  sales2[[2]],\n  weights = price\n)\n\nlaspeyres &lt;- exp(\n  weighted.mean(\n    predict(mdl2, sales2[[1]]) - predict(mdl1, sales2[[1]]),\n    sales2[[1]]$price\n  )\n)\n\npaasche &lt;- exp(\n  weighted.mean(\n    predict(mdl2, sales2[[2]]) - predict(mdl1, sales2[[2]]),\n    sales2[[2]]$price\n  )\n)\n\nfisher &lt;- sqrt(laspeyres * paasche)\n\nMaking the Fisher index with one regression just requires removing the weighted mean from the characteristics variables in the interaction term.\n\ndm &lt;- function(x) {\n  m0 &lt;- eval(\n    substitute(weighted.mean(x[period == 0], price[period == 0])),\n    parent.frame()\n  )\n  m1 &lt;- eval(\n    substitute(weighted.mean(x[period == 1], price[period == 1])),\n    parent.frame()\n  )\n  x - mean(c(m0, m1))\n}\n\nmdl_fisher &lt;- lm(\n  log(price) ~ period +\n    age +\n    area +\n    I(area^2) +\n    amenity_score +\n    period:(dm(age) + dm(area) + dm(area^2) + dm(amenity_score)),\n  sales,\n  weights = price\n)\n\nall.equal(exp(coef(mdl_fisher)[\"period\"]), fisher, check.attributes = FALSE)\n\n[1] TRUE"
  },
  {
    "objectID": "blog/posts/2025/hedonics/index.html#why-two-regressions-can-be-better",
    "href": "blog/posts/2025/hedonics/index.html#why-two-regressions-can-be-better",
    "title": "Hedonic imputation with one regression, not two",
    "section": "Why two regressions can be better",
    "text": "Why two regressions can be better\nAs noted by Aizcorbe (2014, chap. 3), constructing the hedonic-imputation index with two regressions is more flexible than recovering an index from a regression coefficient. The trick of doing it with one regression works because the model is linear; using a non-linear regression model (however rare) may require fitting two models separately. Explicitly using the fitted values from two models to make price relatives also allows for the use of other index-number formulas, not just geometric ones.\nOne reason that I‚Äôve not seen for the two-regression approach is that it‚Äôs also more convenient to calculate product contributions when doing two regressions.\n\nlaspeyres_relatives &lt;- exp(\n  predict(mdl2, sales2[[1]]) - predict(mdl1, sales2[[1]])\n)\ncontrib_laspeyres &lt;- gpindex::geometric_contributions(laspeyres_relatives)\n\npaasche_relatives &lt;- exp(\n  predict(mdl2, sales2[[2]]) - predict(mdl1, sales2[[2]])\n)\ncontrib_paasche &lt;- gpindex::geometric_contributions(paasche_relatives)\n\ncontrib_fisher &lt;- Map(\n  `*`,\n  gpindex::transmute_weights(0, 1)(c(laspeyres, paasche)),\n  list(contrib_laspeyres, contrib_paasche)\n)\n\nplot(\n  density(unlist(contrib_fisher)),\n  main = \"Distribution of sales contributions\"\n)"
  },
  {
    "objectID": "blog/posts/2025/hedonics/index.html#footnotes",
    "href": "blog/posts/2025/hedonics/index.html#footnotes",
    "title": "Hedonic imputation with one regression, not two",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis type of regression model comes up in the literature on program evaluation as a way to estimate average treatment effects; see, e.g., Wooldridge (2002, sec. 18.3.1).‚Ü©Ô∏é\nThese standard error are approximate when the mean of the characteristics is calculated from the sample of data used to the fit the model.‚Ü©Ô∏é"
  }
]